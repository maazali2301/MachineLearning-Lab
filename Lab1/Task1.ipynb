{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "902f17f0",
   "metadata": {},
   "source": [
    "# Train-Test Split and Cross Validation - using numpy\n",
    "## Maaz Ali Nadeem - i200452 - BS (AI-5J) - Machine Learning Lab Task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "658d86e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "bc16b225",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set a ratio for the size of train and test dataframes\n",
    "test_ratio = 0.3\n",
    "train_ratio = 0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "14b22b44",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Python_PAI\\lib\\site-packages\\pandas\\util\\_decorators.py:311: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  return func(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       " 1.0    69\n",
       "-1.0    31\n",
       "Name: 0, dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ecg_df = pd.read_csv(\"ECG200_TRAIN.csv\", delimiter=\"  \", header=None)\n",
    "ecg_df[0].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c307db9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_label1 = ecg_df[ecg_df[0]==1]\n",
    "df_label2 = ecg_df[ecg_df[0]==-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "421d5fc9",
   "metadata": {},
   "source": [
    "## K-Folds Verification (k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1875e5c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 3\n",
    "df1 = []\n",
    "df2 = []\n",
    "\n",
    "for i in range(k):\n",
    "    df1.append(pd.DataFrame(np.array_split(df_label1, k)[i]))\n",
    "    df2.append(pd.DataFrame(np.array_split(df_label2, k)[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "44ccd5de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold # 0 Accuracy :  88.23529411764706\n",
      "Fold # 0 F1 Score :  85.83333333333333\n",
      " \n",
      "Fold # 1 Accuracy :  72.72727272727273\n",
      "Fold # 1 F1 Score :  68.57142857142857\n",
      " \n",
      "Fold # 2 Accuracy :  78.78787878787878\n",
      "Fold # 2 F1 Score :  72.26890756302521\n",
      " \n"
     ]
    }
   ],
   "source": [
    "for i in range(k):\n",
    "    \n",
    "    #Let the kth splitted dataframe be test dataset (all other dataset segments are training)\n",
    "    test_df = df1[i].append(df2[i], ignore_index = True)\n",
    "    \n",
    "    #all other sets are training\n",
    "    train_df = pd.DataFrame(columns = ecg_df.columns)\n",
    "    for j in range(k):\n",
    "        if (j == i):\n",
    "            continue\n",
    "        train_df = train_df.append(df1[j], ignore_index=True)\n",
    "        train_df = train_df.append(df2[j], ignore_index=True)\n",
    "        \n",
    "    X_train = train_df.drop([0], axis=1)\n",
    "    y_train = train_df[0]\n",
    "    X_test = test_df.drop([0], axis=1)\n",
    "    y_test = test_df[0]\n",
    "    \n",
    "    clf = tree.DecisionTreeClassifier()\n",
    "    clf = clf.fit(X_train, y_train)\n",
    "    pred=clf.predict(X_test)\n",
    "    print (\"Fold #\", i, \"Accuracy : \" , accuracy_score(y_test,pred)*100) \n",
    "    print (\"Fold #\", i, \"F1 Score : \",f1_score(y_test, pred, average='macro')*100)\n",
    "    print (\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c5e5dfa",
   "metadata": {},
   "source": [
    "## Evaluation with Sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "9eeb0513",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn import tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "a93174b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = ecg_df.drop(0, axis=1)\n",
    "y = ecg_df[0]\n",
    "X_train, X_test, y_train, y_test =train_test_split(X, y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "706aba5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy :  83.33333333333334\n",
      "F1 Score :  80.74454428754814\n"
     ]
    }
   ],
   "source": [
    "clf = tree.DecisionTreeClassifier()\n",
    "clf = clf.fit(X_train, y_train)\n",
    "pred=clf.predict(X_test)\n",
    "print (\"Accuracy : \" , accuracy_score(y_test,pred)*100)  \n",
    "print(\"F1 Score : \",f1_score(y_test, pred, average='macro')*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba8ca5c",
   "metadata": {},
   "source": [
    "## Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b3e960",
   "metadata": {},
   "source": [
    "The accuracy isnt a good measure of evaluation when dealing with imbalanced data, in this case, the model could \n",
    "easily evaluate upto 70% accuracy just by predicting the class label '1' all the time (which is a poor approach)\n",
    "Therefore i used the F-1 measure which combines both precision and recall scores to evaluate model performance. \n",
    "\n",
    "The F-1 scores of the K-folds approach average upto 75.33 while the Sklearn approach yields 80.7. These values fluctuate\n",
    "more often for Sklearn as the dataset is picked randomly and may have a difference in label proportion each time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f59c075a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train_test_dataset(df, train_ratio):\n",
    "    \n",
    "#     #DataFrame size to generate\n",
    "#     train_size = int(len(df) * train_ratio)\n",
    "#     test_ratio = int(len(df) * 1 - train_ratio)\n",
    "    \n",
    "#     #Sort the DataFrame according to different labels\n",
    "#     df = df.sort_values(by=[0])\n",
    "    \n",
    "#     df_train = pd.DataFrame(columns=[df.columns])\n",
    "#     df_test = pd.DataFrame(columns=[df.columns])\n",
    "    \n",
    "#     visited_num = []\n",
    "    \n",
    "#     while (not len(df_train) == train_size):\n",
    "#         val = random.randint(0, len(df)-1)\n",
    "#         if (val in visited_num):\n",
    "#             continue\n",
    "#         visited_num.append(val)\n",
    "#         df_train.append(df.loc[val])\n",
    "        \n",
    "#     if (df_train[0].value_counts.tolist()[0]/df_train[0].value_counts.tolist()[1] - df[0].value_counts.tolist()[0]/df[0].value_counts.tolist()[1] <= 8):\n",
    "#         for i in range(0, len(df)):\n",
    "#             if i in visited_num:\n",
    "#                 continue\n",
    "#             else:\n",
    "#                 df_test.append(df.loc[i])\n",
    "                \n",
    "#         return df_train, df_test\n",
    "#     else:\n",
    "#         train_test_dataset(df, train_ratio)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
